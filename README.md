# üöÄ Pipeline de Transforma√ß√£o de Dados ‚Äî Lighthouse Checkpoint 3

## 1. Vis√£o Geral do Projeto

Este projeto foi desenvolvido como parte do desafio **Lighthouse Checkpoint 3** da Indicium, com o objetivo de construir uma arquitetura anal√≠tica moderna, baseada no paradigma **Lakehouse** e nas boas pr√°ticas de engenharia e modelagem de dados.

A proposta consiste na ingest√£o, transforma√ß√£o e organiza√ß√£o de dados oriundos de m√∫ltiplas fontes ‚Äî uma API REST e um banco de dados SQL Server ‚Äî utilizando ferramentas robustas e amplamente adotadas pelo mercado:

- **Databricks** como motor anal√≠tico e plataforma unificada de dados
- **dbt (Data Build Tool)** para orquestra√ß√£o e versionamento das transforma√ß√µes
- **Delta Lake** para garantir escalabilidade, performance e governan√ßa dos dados
- **Databricks Workflows** para orquestra√ß√£o e agendamento automatizado da pipeline

A arquitetura foi concebida para refletir os princ√≠pios de **modularidade, reprodutibilidade, testes, versionamento e documenta√ß√£o**, facilitando tanto o desenvolvimento colaborativo quanto a manuten√ß√£o em produ√ß√£o.

---

## 2. Objetivos T√©cnicos

- ‚úÖ Construir um pipeline de dados escal√°vel utilizando **Databricks Lakehouse** e **Delta Lake**
- ‚úÖ Realizar ingest√£o de m√∫ltiplas fontes de dados (SQL Server e API REST)
- ‚úÖ Implementar uma modelagem dimensional baseada no framework **Kimball**
- ‚úÖ Organizar os dados em camadas **Bronze**, **Silver** e **Gold**
- ‚úÖ Aplicar boas pr√°ticas de desenvolvimento com **dbt** (modulariza√ß√£o, testes, documenta√ß√£o)
- ‚úÖ Criar **dimens√µes** e **fatos** com foco em an√°lise de vendas e performance
- ‚úÖ Agendar a execu√ß√£o automatizada da pipeline com **Databricks Workflows (Jobs YAML)**
- ‚úÖ Gerar documenta√ß√£o naveg√°vel e autoatualiz√°vel com `dbt docs`

---

## 3. Instru√ß√µes de Execu√ß√£o Local

### 3.1 Pr√©-requisitos

Para executar este projeto localmente, voc√™ precisar√° dos seguintes itens instalados:

- [Python 3.10 ou 3.11](https://www.python.org/)
- [pip](https://pip.pypa.io/)
- [dbt-databricks](https://docs.getdbt.com/reference/warehouse-profiles/databricks-profile)
- [Git](https://git-scm.com/)
- Conta gratuita no [Databricks Community Edition](https://community.cloud.databricks.com/) com um Warehouse Serverless (Starter Warehouse)

---

### 3.2 Clonando o Reposit√≥rio

```bash
git clone https://github.com/alerodriguessf/lighthouse_desafio03_alexandrersf
cd lighthouse_desafio03_alexandrersf
````

---

### 3.3 Criando o Ambiente Virtual

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

---

### 3.4 Instalando as Depend√™ncias

```bash
pip install -r requirements.txt
```

> O arquivo `requirements.txt` cont√©m as depend√™ncias essenciais, incluindo:
>
> * `dbt-databricks`
> * `pyspark`
> * `pydantic` (compat√≠vel com dbt >= 1.10)

---

### 3.5 Configurando o Profile do dbt

Crie o arquivo de perfil do dbt em `~/.dbt/profiles.yml` com as suas credenciais do Databricks:

```yaml
dbt_checkpoint3_dw:
  target: dev
  outputs:
    dev:
      type: databricks
      catalog: ted_dev
      schema: silver
      host: https://<sua-instancia>.cloud.databricks.com
      http_path: /sql/1.0/warehouses/<seu-warehouse-id>
      token: <seu-personal-access-token>
```

> Substitua os campos `<...>` pelas informa√ß√µes da sua inst√¢ncia Databricks.
> Voc√™ pode encontrar o `http_path` e o `token` na interface web do Databricks.

---

### 3.6 Executando os Comandos dbt

Com o ambiente ativado e o profile configurado, voc√™ pode executar os seguintes comandos:

#### Instalar depend√™ncias do projeto:

```bash
dbt deps
```

#### Compilar e executar os modelos:

```bash
dbt run
```

#### Rodar os testes de qualidade de dados:

```bash
dbt test
```

#### Gerar a documenta√ß√£o naveg√°vel:

```bash
dbt docs generate
dbt docs serve
```

> A documenta√ß√£o ser√° aberta automaticamente no seu navegador em `http://localhost:8000`.

---

### ‚úîÔ∏è Pronto!

Se todos os passos forem seguidos corretamente, o dbt executar√° as transforma√ß√µes e criar√° as tabelas `staging`, `dimensions` e `facts` no seu ambiente Databricks.


Perfeito! Com base na estrutura visual do projeto DBT que voc√™ compartilhou (imagem `estrutura dbt.png`), nos requisitos do desafio e no hist√≥rico do checkpoint 2, aqui est√° a pr√≥xima se√ß√£o detalhada em Markdown para o seu `README.md`:

---

Claro! Aqui est√° a **revis√£o da se√ß√£o 3. Organiza√ß√£o e Modelagem no DBT**, alinhada com a estrutura, tom e rigor t√©cnico estabelecidos na se√ß√£o anterior (3.6). Essa vers√£o evita exageros nos emojis, mant√©m uma linguagem clara e profissional, e segue uma estrutura coesa com o restante do `README.md`.

---

## 3. Organiza√ß√£o e Modelagem no DBT

A modelagem dos dados foi implementada utilizando o [dbt (data build tool)](https://www.getdbt.com/), com base na arquitetura em camadas (staging e marts), alinhada √†s boas pr√°ticas de engenharia de dados e modelagem dimensional. Essa separa√ß√£o permite maior controle, reusabilidade e governan√ßa dos dados transformados.

> A documenta√ß√£o completa dos modelos DBT pode ser consultada online:
> [https://checkpoint3-alexandrersf.netlify.app/#!/overview](https://checkpoint3-alexandrersf.netlify.app/#!/overview)

### 3.1 Estrutura dos diret√≥rios

A estrutura dos modelos no projeto segue a conven√ß√£o recomendada pelo dbt, dividida em duas camadas principais:

```
models/
‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îú‚îÄ‚îÄ stg_sales_order_header.sql
‚îÇ   ‚îú‚îÄ‚îÄ stg_sales_order_detail.sql
‚îÇ   ‚îú‚îÄ‚îÄ stg_sales_customer_info.sql
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml
‚îú‚îÄ‚îÄ marts/
‚îÇ   ‚îú‚îÄ‚îÄ dim_date.sql
‚îÇ   ‚îú‚îÄ‚îÄ dim_product.sql
‚îÇ   ‚îú‚îÄ‚îÄ fact_sales_order.sql
‚îÇ   ‚îú‚îÄ‚îÄ fact_sales_summary_monthly.sql
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ schema.yml
```

* **Staging**: respons√°vel por tratamentos leves, normaliza√ß√£o, padroniza√ß√£o de tipos e nomenclaturas.
* **Marts**: representa os modelos anal√≠ticos, contendo as tabelas fato e dimens√µes consumidas em an√°lises.

### 3.2 Modelos de Staging

Os modelos de staging t√™m como fun√ß√£o:

* Isolar transforma√ß√µes iniciais
* Aplicar *casting* de tipos de dados
* Uniformizar nomes de colunas
* Lidar com dados nulos e inconsist√™ncias

Exemplos:

* `stg_sales_order_header.sql`
* `stg_sales_order_detail.sql`
* `stg_sales_person_details.sql`
* `stg_sales_ship_method.sql`

Cada um desses modelos se conecta diretamente √†s tabelas Delta da camada Bronze do Databricks.

### 3.3 Modelos de Marts

Os modelos da camada marts realizam agrega√ß√µes, jun√ß√µes e enriquecimentos dos dados, sendo organizados em:

* **Tabelas Fato**:

  * `fact_sales_order.sql`: granularidade ao n√≠vel de item de pedido
  * `fact_sales_summary_monthly.sql`: resumo mensal de vendas por cliente e vendedor

* **Tabelas Dimens√£o**:

  * `dim_date.sql`: calend√°rio base para an√°lises temporais
  * `dim_product.sql`: metadados de produtos
  * `dim_customer.sql`: identifica√ß√£o de clientes
  * `dim_salesperson.sql`: equipe comercial
  * `dim_ship_method.sql`: m√©todos de envio

### 3.4 Testes e valida√ß√µes

Os arquivos `schema.yml` em cada camada cont√™m:

* Testes de integridade: `not_null`, `unique`, `relationships`
* Documenta√ß√£o de colunas e modelos
* Tags para facilitar a organiza√ß√£o de execu√ß√µes no CI/CD

Exemplo de teste implementado:

```yaml
- name: dim_product
  description: "Dimens√£o de produtos com enriquecimento"
  tests:
    - unique:
        column_name: product_id
    - not_null:
        column_name: product_id
```

### 3.5 Gera√ß√£o e publica√ß√£o da documenta√ß√£o

A documenta√ß√£o dos modelos foi gerada com:

```bash
dbt docs generate
```

E publicada utilizando o Netlify, permitindo navega√ß√£o interativa entre tabelas, colunas, depend√™ncias e descri√ß√µes.

Link p√∫blico: [checkpoint3-alexandrersf.netlify.app](https://checkpoint3-alexandrersf.netlify.app/#!/overview)

### 3.6 Execu√ß√£o dos modelos DBT

A execu√ß√£o local dos modelos pode ser feita com os seguintes comandos:

```bash
# Instalar depend√™ncias
dbt deps

# Executar todos os modelos
dbt run

# Executar apenas os modelos da camada mart
dbt run --select marts

# Gerar documenta√ß√£o
dbt docs generate

# Abrir visualmente a documenta√ß√£o local
dbt docs serve
```

> Obs.: O projeto est√° integrado √† orquestra√ß√£o Databricks, e os comandos `dbt deps` e `dbt run` s√£o acionados automaticamente via pipeline.

---

Perfeito. Com base no YAML do pipeline que voc√™ forneceu, na estrutura do projeto e nas boas pr√°ticas de orquestra√ß√£o com Databricks, aqui est√° a **Se√ß√£o 4 ‚Äì Orquestra√ß√£o no Databricks**, escrita em Markdown e com linguagem alinhada √†s se√ß√µes anteriores do `README.md`.

---

## 4. Orquestra√ß√£o no Databricks

Para garantir reprodutibilidade, modularidade e automa√ß√£o, a pipeline foi orquestrada diretamente no **Databricks Workflows**, utilizando uma defini√ß√£o YAML compat√≠vel com a estrutura declarativa do Databricks CLI v2.

A orquestra√ß√£o contempla as etapas cr√≠ticas da transforma√ß√£o de dados no Lakehouse:

1. Convers√£o dos arquivos Parquet da API e SQL Server para tabelas Delta (camada Bronze)
2. Execu√ß√£o do pipeline dbt (staging e marts)
3. Parametriza√ß√£o flex√≠vel via vari√°veis de ambiente

### 4.1 Estrutura geral do pipeline

O workflow √© composto por **tr√™s tarefas principais**:

| Task Key                     | Tipo     | Descri√ß√£o                                                               |
| ---------------------------- | -------- | ----------------------------------------------------------------------- |
| `delta_conversion_api`       | Notebook | Converte os arquivos `.parquet` da API em tabelas Delta (camada Bronze) |
| `delta_conversion_sqlserver` | Notebook | Converte os arquivos do SQL Server em Delta                             |
| `dbt_run`                    | DBT Task | Executa o pipeline dbt ap√≥s a convers√£o das tabelas                     |

### 4.2 Hierarquia e depend√™ncias

O pipeline √© executado diariamente e segue a seguinte ordem de execu√ß√£o:

```
delta_conversion_api          ‚îÄ‚îê
                              ‚îú‚îÄ‚îÄ>  dbt_run
delta_conversion_sqlserver    ‚îò
```

Essa estrutura garante que o pipeline dbt s√≥ seja executado ap√≥s a ingest√£o e convers√£o completa dos dados em Delta Lake.

### 4.3 Par√¢metros e ambientes

O pipeline utiliza **par√¢metros externos** para permitir reuso em diferentes ambientes e cat√°logos:

```yaml
parameters:
  - name: DATABRICKS_CATALOG
    default: ted_dev
  - name: DATABRICKS_SCHEMA_RAW
    default: bronze
  - name: DATABRICKS_SCHEMA_STAGING
    default: silver
  - name: DATABRICKS_SCHEMA_MARTS
    default: gold
  - name: DATABRICKS_VOLUME
    default: raw
```

Al√©m disso, foi configurado um **ambiente padr√£o (`dbt-default`)** com as depend√™ncias necess√°rias:

```yaml
environments:
  - environment_key: dbt-default
    spec:
      client: "2"
      dependencies:
        - dbt-databricks>=1.10.4
```

### 4.4 Execu√ß√£o de comandos dbt no Databricks

A tarefa `dbt_run` executa os seguintes comandos:

```bash
dbt deps --vars '{...}'      # Instala depend√™ncias
dbt run --vars '{...}'       # Executa todos os modelos
```

As vari√°veis passadas ao `--vars` s√£o resolvidas a partir dos par√¢metros definidos no job, permitindo total desacoplamento e controle da execu√ß√£o.

### 4.5 Versionamento e reprodutibilidade

A defini√ß√£o do pipeline est√° versionada em Git:

```yaml
git_source:
  git_url: https://github.com/alerodriguessf/lighthouse_desafio03_alexandrersf-
  git_provider: gitHub
  git_branch: databricks
```

Isso garante que cada execu√ß√£o do workflow esteja atrelada a um estado espec√≠fico do projeto, assegurando rastreabilidade e consist√™ncia.

Perfeito! Seguindo o mesmo padr√£o de clareza, profundidade e consist√™ncia com as se√ß√µes anteriores, aqui est√° a **Se√ß√£o 5 ‚Äì Documenta√ß√£o, Visualiza√ß√£o e Entreg√°veis**, j√° em Markdown para ser colada diretamente no seu `README.md`.

---

## 5. Documenta√ß√£o, Visualiza√ß√£o e Entreg√°veis

### 5.1 Documenta√ß√£o t√©cnica (dbt Docs)

Todos os modelos desenvolvidos no `dbt` foram documentados utilizando descri√ß√µes claras para tabelas e colunas, incluindo:

* Fonte dos dados (upstream)
* L√≥gica de transforma√ß√£o
* Regras de neg√≥cio aplicadas
* Dicion√°rio de dados (coluna a coluna)

A documenta√ß√£o gerada pode ser acessada publicamente atrav√©s do link:

üîó **[Acesse a documenta√ß√£o dbt](https://checkpoint3-alexandrersf.netlify.app/#!/overview)**

Essa interface √© √∫til para:

* Validar o fluxo de dados e depend√™ncias entre modelos (`Lineage`)
* Consultar os testes aplicados (como `not_null`, `unique`)
* Garantir a governan√ßa da informa√ß√£o e transpar√™ncia anal√≠tica

---

### 5.2 Outputs gerados no Lakehouse

Ao final da execu√ß√£o da pipeline, as seguintes **camadas e tabelas** s√£o criadas automaticamente no cat√°logo `ted_dev`:

#### Bronze

* `raw_api_<nome_tabela>_db`: Tabelas de origem da API REST
* `raw_sqlserver_<nome_tabela>_db`: Tabelas extra√≠das do banco SQL Server

#### Silver (Staging)

* Modelos intermedi√°rios com limpeza, renomea√ß√£o e valida√ß√£o de schema, todos prefixados com `stg_`

#### Gold (Marts)

* **Dimens√µes:**

  * `dim_customer`
  * `dim_date`
  * `dim_product`
  * `dim_salesperson`
  * `dim_ship_method`

* **Fatos:**

  * `fact_sales_order`
  * `fact_sales_summary_monthly`
  * (Outras fatos agregadas ou snapshot podem ser adicionadas conforme necessidade anal√≠tica)

---

### 5.3 Entreg√°veis do Projeto

| Item                         | Status      | Localiza√ß√£o                                                                                      |
| ---------------------------- | ----------- | ------------------------------------------------------------------------------------------------ |
| Pipeline de ingest√£o Meltano | ‚úÖ Conclu√≠do | `entrypoint.sh`, `meltano.yml`, plugins e configura√ß√£o `.env`                                    |
| Convers√£o para Delta Lake    | ‚úÖ Conclu√≠do | Notebooks Databricks em `/scripts_aux/`                                                          |
| Projeto `dbt` com testes     | ‚úÖ Conclu√≠do | Diret√≥rio `/models`, `dbt_project.yml`, `schema.yml`, `profiles.yml`                             |
| Orquestra√ß√£o no Databricks   | ‚úÖ Conclu√≠do | Arquivo YAML versionado com defini√ß√£o do pipeline                                                |
| Documenta√ß√£o dbt             | ‚úÖ Publicado | [checkpoint3-alexandrersf.netlify.app](https://checkpoint3-alexandrersf.netlify.app/#!/overview) |

---

### 5.4 Considera√ß√µes finais

O projeto foi constru√≠do com foco em:

* Transpar√™ncia e modularidade do processo anal√≠tico
* Reprodutibilidade e versionamento de ponta a ponta
* Escalabilidade para novas fontes e dom√≠nios de dados

Todos os componentes s√£o extens√≠veis e podem ser facilmente adaptados para atender a novos requisitos de neg√≥cio ou expans√£o da arquitetura de dados.

Claro! Abaixo est√° a **Se√ß√£o 6 ‚Äì Contato e Cr√©ditos**, finalizando o seu `README.md` com clareza e profissionalismo:

---

## 6. Contato e Cr√©ditos

Este projeto foi desenvolvido por **Alexandre R. Silva Filho** como parte do programa **Lighthouse** da [Indicium Tech](https://indicium.tech), integrando conhecimentos de engenharia e modelagem de dados, orquestra√ß√£o de pipelines e melhores pr√°ticas em arquitetura de dados moderna.

### üë§ Autor

* **Nome:** Alexandre R. Silva Filho
* **Email:** [alexandre.filho@indicium.tech](mailto:alexandre.filho@indicium.tech)
* **LinkedIn:**[https://www.linkedin.com/in/alerodriguessf](https://www.linkedin.com/in/alexandrersf/)
* **GitHub:** [github.com/alerodriguessf](https://github.com/alerodriguessf)


### üìÑ Licen√ßa

Este reposit√≥rio √© de uso educacional e n√£o possui restri√ß√µes de licen√ßa para reprodu√ß√£o pessoal ou testes. Para fins comerciais ou reuso corporativo, recomenda-se an√°lise pr√©via e adapta√ß√£o conforme necessidade.

---





