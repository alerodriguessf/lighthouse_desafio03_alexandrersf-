
# üöÄ Pipeline de Transforma√ß√£o de Dados ‚Äî Lighthouse Checkpoint 3

## 1. Vis√£o Geral do Projeto

Este projeto consolida os desafios **Lighthouse Checkpoint 2 e 3** da Indicium, implementando uma **pipeline de dados completa e moderna**. A solu√ß√£o abrange desde a ingest√£o de dados de m√∫ltiplas fontes at√© sua transforma√ß√£o e disponibiliza√ß√£o para an√°lise em uma arquitetura **Lakehouse**.

A pipeline realiza a ingest√£o de dados de um **banco de dados relacional (MSSQL)** e de uma **API REST** utilizando uma stack conteinerizada com **Meltano e Docker**. Posteriormente, esses dados s√£o transformados, modelados e orquestrados no **Databricks** com **dbt (Data Build Tool)**, seguindo as melhores pr√°ticas de engenharia de dados.

O objetivo final √© entregar um ecossistema de dados **confi√°vel, modular, escal√°vel e automatizado**, pronto para suportar an√°lises de neg√≥cio complexas.

---

## 2. Arquitetura da Solu√ß√£o

A arquitetura foi desenhada para ser desacoplada e robusta, dividindo o fluxo de dados em etapas claras e gerenci√°veis.

1.  **Extra√ß√£o e Carga (EL):** O **Meltano**, orquestrado dentro de um cont√™iner **Docker**, extrai dados das fontes (MSSQL e API). Os dados s√£o materializados como arquivos **Parquet**.
2.  **Upload para o Lakehouse:** O **Databricks CLI**, tamb√©m no cont√™iner, carrega os arquivos Parquet para o Databricks File System (DBFS).
3.  **Camada Bronze:** Notebooks no Databricks convertem os dados Parquet para o formato **Delta Lake**, criando as tabelas da camada Bronze e garantindo transa√ß√µes ACID, versionamento e performance.
4.  **Camadas Silver e Gold (T):** O **dbt** assume o controle para executar as transforma√ß√µes. Ele l√™ os dados da camada Bronze e aplica regras de neg√≥cio, limpeza e modelagem dimensional (Kimball) para criar as camadas Silver (staging) e Gold (marts).
5.  **Orquestra√ß√£o:** O **Databricks Workflows** automatiza todo o processo, desde a convers√£o para Delta at√© a execu√ß√£o dos modelos dbt, garantindo que os dados sejam atualizados de forma agendada e confi√°vel.

### üîß Componentes T√©cnicos

| Componente | Papel na Pipeline | Etapa |
| :--- | :--- | :--- |
| `Meltano` | Extrai dados de fontes diversas com seus conectores (`taps`) | Ingest√£o |
| `Docker` | Cria ambiente de ingest√£o reprodut√≠vel e isolado | Ingest√£o |
| `Target Parquet` | Armazena dados extra√≠dos em formato colunar otimizado | Ingest√£o |
| `Databricks CLI` | Faz o upload dos dados brutos para o Lakehouse | Ingest√£o |
| `Databricks Notebooks` | Converte Parquet em tabelas Delta (Camada Bronze) | Orquestra√ß√£o |
| `dbt (Data Build Tool)` | Orquestra, testa e documenta as transforma√ß√µes SQL | Transforma√ß√£o |
| `Delta Lake` | Garante governan√ßa, performance e confiabilidade aos dados | Todas |
| `Databricks Workflows`| Agenda e executa a pipeline completa de forma automatizada | Orquestra√ß√£o |

---

## 3. Configura√ß√£o e Execu√ß√£o

Para executar a pipeline completa localmente, siga as etapas abaixo.

### 3.1 Pr√©-requisitos

* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (v4.x+)
* [Git](https://git-scm.com/)
* [Python 3.10 ou 3.11](https://www.python.org/) e `pip`
* Acesso a um workspace **Databricks** (Community Edition ou superior)
* Credenciais para o banco **MSSQL** e para a **API REST**

### 3.2 Clonar o Reposit√≥rio

```bash
git clone [https://github.com/alerodriguessf/lighthouse_desafio03_alexandrersf](https://github.com/alerodriguessf/lighthouse_desafio03_alexandrersf)
cd lighthouse_desafio03_alexandrersf
````

### 3.3 Configurar Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto a partir do modelo `.env.save`. Este arquivo centraliza todas as credenciais e configura√ß√µes.

```env
# CREDENCIAIS DE INGEST√ÉO (CHECKPOINT 2)
# MSSQL
TAP_MSSQL_HOST=your_mssql_host
TAP_MSSQL_PORT=1433
TAP_MSSQL_USER=your_user
TAP_MSSQL_PASSWORD=your_password
TAP_MSSQL_DATABASE=AdventureWorks2022

# API
API_HOST=[https://your-api-url.com](https://your-api-url.com)
API_USER=your_api_user
API_PASSWORD=your_api_password

# CREDENCIAIS DO DATABRICKS (PARA INGEST√ÉO E DBT)
DATABRICKS_HOST=[https://your-databricks-instance.cloud.databricks.com](https://your-databricks-instance.cloud.databricks.com)
DATABRICKS_TOKEN=your_pat_token
```

> üîê **Importante**: N√£o versione este arquivo com Git. Ele j√° est√° inclu√≠do no `.gitignore`.

### 3.4 Etapa 1: Ingest√£o de Dados (Meltano & Docker)

Esta etapa extrai os dados das fontes e os carrega como arquivos Parquet no Databricks.

**1. Construir a Imagem Docker:**

```bash
docker build -t lighthouse-ingestion-pipeline .
```

**2. Executar o Cont√™iner de Ingest√£o:**

```bash
docker run --env-file .env lighthouse-ingestion-pipeline
```

Este comando executa o script `entrypoint.sh`, que realiza a extra√ß√£o via Meltano e o upload dos arquivos `.parquet` para o DBFS.

### 3.5 Etapa 2: Transforma√ß√£o de Dados (dbt)

Com os dados brutos no Databricks, esta etapa executa as transforma√ß√µes para criar os modelos anal√≠ticos.

**1. Criar o Ambiente Virtual:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ou
venv\Scripts\activate     # Windows
```

**2. Instalar as Depend√™ncias:**

```bash
pip install -r requirements.txt
```

**3. Configurar o Profile do dbt:**
Crie o arquivo `~/.dbt/profiles.yml` com suas credenciais do Databricks. O dbt o utilizar√° para se conectar ao seu workspace.

```yaml
dbt_checkpoint3_dw:
  target: dev
  outputs:
    dev:
      type: databricks
      catalog: ted_dev
      schema: silver
      host: <seu-databricks-host> # Ex: [https://adb-....cloud.databricks.com](https://adb-....cloud.databricks.com)
      http_path: /sql/1.0/warehouses/<seu-warehouse-id>
      token: <seu-personal-access-token>
```

**4. Executar os Comandos dbt:**

```bash
# Instalar depend√™ncias do projeto dbt (se houver)
dbt deps

# Executar os modelos (camadas Bronze -> Silver -> Gold)
dbt run

# Rodar os testes de qualidade de dados
dbt test

# Gerar e servir a documenta√ß√£o localmente
dbt docs generate
dbt docs serve
```

-----

## 4\. Organiza√ß√£o e Modelagem no DBT

A modelagem dos dados foi implementada utilizando o dbt com base na arquitetura em camadas (staging e marts), alinhada √†s boas pr√°ticas de engenharia de dados e modelagem dimensional.

> A documenta√ß√£o completa e naveg√°vel dos modelos DBT pode ser consultada online:
> **[https://checkpoint3-alexandrersf.netlify.app/\#\!/overview](https://checkpoint3-alexandrersf.netlify.app/#!/overview)**

### 4.1 Estrutura dos diret√≥rios

```
models/
‚îú‚îÄ‚îÄ staging/            # Limpeza, padroniza√ß√£o e normaliza√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ stg_sales_order_header.sql
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ marts/              # Modelos de neg√≥cio (dimens√µes e fatos)
‚îÇ   ‚îú‚îÄ‚îÄ dim_product.sql
‚îÇ   ‚îú‚îÄ‚îÄ fact_sales_order.sql
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

  * **Staging (Camada Silver):** Isola transforma√ß√µes iniciais, aplica casting, uniformiza nomes e lida com inconsist√™ncias.
  * **Marts (Camada Gold):** Representa os modelos anal√≠ticos, contendo as tabelas fato e dimens√µes prontas para consumo.

### 4.2 Testes e Valida√ß√µes

Os arquivos `schema.yml` em cada diret√≥rio cont√™m testes de integridade (`not_null`, `unique`, `relationships`), documenta√ß√£o de colunas e tags para organiza√ß√£o das execu√ß√µes.

-----

## 5. Orquestra√ß√£o e Reprodutibilidade com Databricks Workflows

A automa√ß√£o da pipeline √© gerenciada pelo **Databricks Workflows**, garantindo que os dados sejam processados de forma agendada, confi√°vel e na ordem correta. Toda a configura√ß√£o do workflow est√° definida de forma declarativa no arquivo `databricks_pipeline.yml`, localizado na raiz deste reposit√≥rio.

Essa abordagem permite que a orquestra√ß√£o seja versionada junto com o c√≥digo e facilmente replicada.

### 5.1 Estrutura e Depend√™ncias do Workflow

O workflow √© composto por tarefas que executam notebooks e o pipeline dbt. As depend√™ncias entre elas garantem a integridade do fluxo de ponta a ponta.

| Chave da Tarefa | Tipo de Tarefa | Descri√ß√£o |
| :--- | :--- | :--- |
| `delta_conversion_api` | Notebook | Executa o script em `/scripts_aux/` para converter os dados da API (Parquet) em tabelas Delta na camada Bronze. |
| `delta_conversion_sqlserver` | Notebook | Executa o script em `/scripts_aux/` para converter os dados do SQL Server (Parquet) para o formato Delta. |
| `dbt_run` | Tarefa DBT | Ap√≥s a conclus√£o das convers√µes, executa `dbt deps` e `dbt run` para atualizar as camadas Silver e Gold. |

A sequ√™ncia de execu√ß√£o √© a seguinte:

```
[delta_conversion_api]       ‚îÄ‚îê
                               ‚îú‚îÄ‚îÄ>  [dbt_run]
[delta_conversion_sqlserver] ‚îò
```

### 5.2 Como Implantar o Workflow no Databricks

Em vez de usar a CLI, voc√™ pode criar o job diretamente na interface do Databricks de forma simples:

1.  **Acesse o seu Workspace Databricks** e navegue at√© a se√ß√£o **Workflows**.
2.  Clique no bot√£o **Create Job**.
3.  D√™ um nome ao seu Job (ex: `lighthouse_pipeline_dbt`).
4.  Na tela de configura√ß√£o da primeira tarefa, procure e clique na op√ß√£o **Edit YAML**.
5.  **Abra o arquivo `databricks_pipeline.yml`** que est√° na raiz do reposit√≥rio em sua m√°quina local.
6.  **Copie todo o conte√∫do** do arquivo.
7.  **Cole o conte√∫do** no editor YAML dentro do Databricks.
8.  Clique em **Save**.

Pronto\! O seu workflow ser√° criado com todas as tarefas, depend√™ncias e configura√ß√µes definidas no arquivo. Agora voc√™ pode execut√°-lo manualmente ou aguardar a execu√ß√£o agendada.

-----

## 6\. Documenta√ß√£o, Visualiza√ß√£o e Entreg√°veis

### 6.1 Documenta√ß√£o T√©cnica (dbt Docs)

Todos os modelos, colunas e fontes foram documentados. A documenta√ß√£o interativa, com linhagem de dados e descri√ß√µes, est√° publicada e pode ser acessada publicamente.

üîó **[Acesse a documenta√ß√£o dbt](https://checkpoint3-alexandrersf.netlify.app/#!/overview)**

### 6.2 Entreg√°veis do Projeto

| Item | Status | Localiza√ß√£o / Artefato |
| :--- | :--- | :--- |
| Pipeline de ingest√£o (Meltano) | ‚úÖ Conclu√≠do | `Dockerfile`, `entrypoint.sh`, `meltano.yml` |
| Convers√£o para Delta Lake | ‚úÖ Conclu√≠do | Notebooks Databricks em `/scripts_aux/` |
| Projeto `dbt` com testes | ‚úÖ Conclu√≠do | Diret√≥rio `/models`, `dbt_project.yml`, `schema.yml` |
| Orquestra√ß√£o no Databricks | ‚úÖ Conclu√≠do | Arquivo YAML versionado com a defini√ß√£o do Job |
| Documenta√ß√£o dbt naveg√°vel | ‚úÖ Publicado | [checkpoint3-alexandrersf.netlify.app](https://checkpoint3-alexandrersf.netlify.app/#!/overview) |

-----

## 7\. Contato e Cr√©ditos

Este projeto foi desenvolvido por **Alexandre R. Silva Filho** como parte do programa **Lighthouse** da [link suspeito removido].

  * **Email:** [alexandre.filho@indicium.tech](mailto:alexandre.filho@indicium.tech)
  * **LinkedIn:** [https://www.linkedin.com/in/alerodriguessf](https://www.linkedin.com/in/alerodriguessf/)
  * **GitHub:** [github.com/alerodriguessf](https://github.com/alerodriguessf)

<!-- end list -->
